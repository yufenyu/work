\section{Design of thread program model}
%这一部分，首先，我们将详细分析Phoenix较差scalability的根本原因，然后针对Phoenix scalability存在的challenge，我们提出可行的解决方案，即构造一种具有较好scalability的thread model。然后详细解释这种新的线程模型的详细设计方案，最后具体的实验部分在section 6
In this section, we first investigate the main factor of Phoenix's limited scalability, 
and then present a new thread model (\myth), which can achieve good scalability.
Finally, the detail design and implementation of \myth is given.


\subsection{Scalability of Phoenix}
%描述Phoenix 的 pool scalability
Phoenix uses shared-memory multiple threads to implement parallelism,
and programs written in Phoenix will start as many threads as the system's cores.
Ideally, adding more threads and cores to the runtime would bring about a linear decrease in execution time.
%However, the parallel scalability of Phoenix is not ideal.
However, Phoenix can not scale as well as expected.
As indicated in figure \ref{fig:phoenix:speedup}, 
when the system exceeds their scalability limitation, 
adding more cores might scale negatively.
That means the time of completing a workload will increase if there are more cores in the system. 
% In general, the parallel scalability of Phoenix is poor.

%使用perf来收集热点函数的执行时间，实验结果显示，在32核情况下，(这里可以给出一个图，16和32核下的热点函数分别是什么，占用多少的百分比分别又是多少？)
%为了深入探究较差scalability的根本原因，
%我们利用Linux perf工具来收集程序的热点函数信息（主要看热点函数占用总运行时间的百分比），
%通过热点函数的分析，查看高核情况下，占用时间最多的运行函数，
%如表\ref{}显示，在16和32核情况下，各个应用程序中占用时间最多的函数。
%从表中可以看出，对一hist, lr, wc, sw，在32核情况下ticket\_spin\_lock的开销非常大，
%针对ticket\_spin\_lock，我们测试各应用程序的占用情况，如图\ref{}所示，
%从图中可以看出，
%随着核数的增多，各应用程序中的ticket\_spin\_lock的占用的开销越来越大，
%特别地，在16核和32核情况下，
%histgram中ticket\_spin\_lock占用的开销最大，分别为71.25\%和40.15\%.
%这表明，16核以上，hist, lr, wc, sm的运行时间主要用于锁的竞争和等待，
%而没有做实际的计算。
%并且随着核数的增多，这种竞争越激烈，导致Phoenix的scalability较差。
%事实上，从前面对Phoenix的分析我们知道，
%Phoenix中采取了划分和barrier两种策略，以避免多个map和reduce对同一个matrix竞争，
%然而实验的结果却显示在8核以上，依然有很激烈的竞争。
%函数的调用图显示，ticket\_spin\_lock几乎全部是由pagefault引起的。
In order to analyze the limited scalability behavior, 
Perf\cite{} is exploited to collect execution time  percent information of hot function. 
We note that the map function is the hottest function with less cores, 
while \_\_ticket\_spin\_lock will become the hottest function with more cores.
%However, the structure data shared by multiple threads in Linux kernel can cause the lock contention, which will degrade the scalability of Phoenix.
%To alleviate the overhead of lock, Phoenix has took two strategies to avoid multiple threads contending the share area.
Figure \ref{fig:phoenix:spinlock} shows the execution time percent of 
\_\_ticket\_spin\_lock on each benchmark. 
We observe that the cost of spinlock increases quickly as the cores number cross a specific value (ie.8).
%And in 32 cores, the \_\_ticket\_spin\_lock needs largest execution time for hist, lr, wc, sw.
Specially, hist with 16 and 32 cores shows that \_\_ticket\_spin\_lock is the function that has largest execution times of 40.15\%  and 71.25\%, respectively. 

\redt{ticket\_spin\_lock is a type of spinlock which is caused by lock contention in Linux kernel.
	In Linux 3.2 kernel, ticket\_spin\_lock is used to ....
	Linux kernel won't scale well on multicores system: 
	that applications will spend an increasing fraction of their time in the kernel as the number of cores increases.
	Experiment results demonstrate that Phoenix will suffer from serious lock contention
	when the cores number exceeds 8. 
	That means most of execution time will be used for waiting but not actual computation.
Call-graph information shows that \_\_ticket\_spin\_lock is caused by pagefault.
That means pagefaults won't scale well on large numbers of cores:	
}


\begin{figure}[!h!t]  
    \centering
    \includegraphics[width=0.45\textwidth]{eps/phoenix_spinlock.eps}
    \caption{Phoenix ticket\_spin\_lock percent}
    \label{fig:phoenix:spinlock}
\end{figure}


%Phoenix采取了划分和barrier的方式，以避免多个线程对共享区域的的竞争，为什么还会存在如此高的spinlock呢，
%\grayt{There is a sense in the community that traditional kernel
%designs won’t scale well on multicore processors: that
%applications will spend an increasing fraction of their time
%in the kernel as the number of cores increases.
%To understand the Linux scalability
%behavior, we analyze the related implementation of Linux
%kernel and exploit performance tools to identify scalability
%bottlenecks.
%}
%and source code analysis

%One reason that widely used operating systems 
%use a lock on the address space is 
%that they use complex index data structures to guarantee O(log n)
%lookup time when a process has many mapped memory
%regions. Linux uses a red-black tree for the regions\cite{linux}. 
%Because the data structures require rebalancing 
%when a memory region is inserted, 
%they protect the entire data structure with a single lock.
%The lock is local to a process address space.
%when the process is using multiple threads 
%then these threads will be able to access
%the address space in parallel, 
%which can cause contention on the lock.
%%随着核数的增多，这种问题会更加突出，最好导致较差的scalability
%\bluet{As the increasing of cores number, 
%	the scalability will be bad.
%}


The mmap() system call is utilized to read in input data. 
Once the user passes the pointer of the mmaped region to runtime as an argument, 
multiple map threads will concurrently cause pagefault in the input data when they invoke map functions.
\redt{All these pagefaults need to access the unique mmap\_sem semaphore, which is local to a process address space. As a result, process's multiple threads concurrently pagefaults will casue contention to the single semaphore.
}
%All these pagefaults will contend the unique mmap\_sem semaphore
%which is used for protecting the address space shared by multiple threads.
One reason why Linux uses the single semaphore on the address space is 
that it needs a red-black tree to guarantee O(log n)
lookup time when a process has many mapped memory regions\cite{linux}. 
%Since the red-black tree requires re-balance when a new memory region is inserted, 
%Linux protect the entire data structure with a single semaphore.
%when the process is using multiple threads, 
%then these threads will be able to access the address space in parallel, 
%Since the semaphore is local to a process address space, process's multiple threads concurrently access will cause contention on the unique semaphore.
The semaphore is a sleep lock and may run into convoying problems,
where waiting threads may get stuck at the end of the wait queue for a long time\cite{Andi2009lmulticore}.
The contention is intense when there are large amounts of threads,
which will lead to the parallel scalability degradation on the benchmarks.
%Therefore, when performing many threads,  the spinlock contention will degrade the parallel scalability performance of the benchmark.

%Data structure private locks can be a problem 
%if the data structure is shared by multiple threads. 
%A standard example here are the mm\_sem read-write semaphore 
%that protects the list of mappings in a process and 
%the pagetable\_lock that protects the pagetable state of a process. 
%These locks are local to a process’ address space. 
%\bluet{In Phoenix, there is one master process,
%and map worker or reduce worker are threads belong to the master process}. 
%However when the process is using multiple threads 
%then these threads will be able to access the address space in parallel,
%which can cause contention on these locks.
%Call-graph information
%and source code analysis show that the two functions are
%called when adding a virtual memory address range into the
%process address space or deleting a virtual memory address range. 




%Unfortunately these programs are often written to start as many threads as the system has CPUs, 
%but when the system is larger than their scalability limit adding more threads might actually scale negatively.
%The first measure is to limit them to the maximum number of threads that they can successfully scale to.
%This of course leaves some of the CPUs idle. 




%The difference between processes and threads under Linux 2.4 is that threads share more parts of their state (address space, file handles etc) than processes, 

\subsection{Scalable thread model}
%这一部分，我们不提生产消费模型，
%为什么要进行地址空间隔离，通过新的线程模型，我们实现了地址空间隔离，隔离之后有什么好处？相比Phoenix，我们这种新的模型会不会带来开销。

Our goal is to enable the pagefaults to be scalable  for many cores.
There are some ways to achieve this target. 
For example, in \cite{Clements2012Scalable},  a scalable operator system is proposed by using ....;
in \cite{},  the authors think process has a better scalability than thread since the process needs not to share the address space with the other processes.
%applications use processes instead of threads can avoid a single shared address space
However, modifying operation system is impracticable and employing the process rather than the thread will make sharing become complicated. 
In order to provide a practicable and simple solution, a novel programming model \myth (Scalable thread) with better scalability,  supporting scalable MapReduce (\myds) compatibly, will be presented in this paper.
Therefore, a basic issue needs to be solved, that is how to enable threads to eliminate contention on the per-process read/write semaphore when multiple threads concurrently run pagefaults.
%This requires addressing a basic problem, how allow pagefaults to eliminate on the per-process read/write semaphore, when multiple threads run concurrently.
%At the same time, we aim at providing an easy-to-use programming abstraction to support scalable MapReduce.
%The proposed \myth can support scalable MapReduce (\myds).


%we persents a new concurrent address space design 
%that eliminates the above sources of contention by applying a new program model and by introducing channel, a way to share data between threads.
%We aim at providing an easy-to-use programming abstraction to support scalable MapReduce.
%With this target, we propose a new thread programming model \myth(Scalable thread).
%We propose a new thread programming model to support efficient pipeline parallelism.
%To address this issue and achieve our goal, we propose a new thread programming model \myth(Scalable thread), which is written in C.
There are two key points in the design of \myth.
Firstly, to avoid the contention on the single semaphore, we confine the threads in \myth to run in separate memory spaces.
Secondly, when using the separate spaces, communication will be challenging since
the threads in \myth can not directly communicate with the other threads like thread based on share space.
So, we give a shared-channel for the threads to communicate with the other threads in \myth.
%Threads in Pthreads share the address space of the process that created it, 
%while threads in \myth have their own address space,
%meaning each thread has a mm\_struct.
%Therefore, thread no need contend with others thread for lock.

%While \myth must use interprocess communication to communicate with the other threads.



\label{sec:pm:thread}
\begin{figure}[htpb]
\input chanapi.tex
\caption{Main functions of \myds thread API.}
\label{fig:api:thread}
\end{figure}

%mapreduce中是如何使用这个简易的模型进行编程和实现的，这个模型潜在的开销是什么
Figure \ref{fig:api:thread} lists main function of managing threads and channels in \myth.
In the case of \myds, at initial stage, the master thread invokes \codet{thread\_alloc} to allocate map threads and reduce threads, and then creates  shared-channel for communication between each pair of map  and reduce threads by invoking \codet{chan\_alloc}.
To set up the shared-channel, the master invokes \codet{chan\_setprod} and \codet{chan\_setcons} to set the map and reduce threads as producers and consumers, respectively. 
The producer sends messages to the shared-channel, and the consumer receives the messages from it,
which is a typical producer-consumer model (we will detail it in Section 4). 
Finally, the master starts all threads to work by invoking \codet{thread\_start}.

Although using \myth can decrease the overhead of contention,
it also takes some extra overhead in comparison to Phoneix. The extra overhead 
is concentrate in the initial stage (we will analyze this overhead in Section 5). 


\subsection{Design of the Channel}
%channel的底层实现，以及它无限制的映射机制，想说明的问题是：不需要等待，且没有过多的malloc和free操作带来的开销。
%Once the producer and consumer thread have been created and channel relationships are set,
In our design, shared-channel is a virtual memory area, called \code{CHAN}, in the producer and consumer address space.
When the producer invoke \codet{chan\_send} to send data, the data will be copyed to the \code{CHAN} area.
And the consumer read the data in the \code{CHAN} area to receive data by invoking \codet{chan\_recv} 
There is a pagetable(\code{ptab}) used to store the mapping between \code{CHAN} memory area and physical address, with each mapping as a corresponding page table entry.

%Once the channel relationships are set up, 
%the producer can invoke \codet{chan\_send} to send key-value to corresponded channel,
%and the reduce workers can receive from the channel by \codet{chan\_recv}.
%In order to avoid the producer waiting when the channel buffer is full,
%we design an unbounded size of communication buffer for channel.
%That means a sender can send any number of messages without blocking or waiting.
%As a result, system can achieve high throughput.
%Unboundedness goal is the key to achieve high throughput.
%这个特性的好处


%无边界的channel的实现，依赖于底层的extend机制，
%它允许将channel buffer区域重新映射到一块新的物理地址，
%并且不影响consumer对旧的物理的读取。
%channel buffer对应producer和consumer地址空间中一块区域，
%我们称之为\codet{CHAN}区域，有一个pagetable（\codet{ptab}）用于管理该区域的物理地址映射。
\begin{figure}[!h!t]  
	\centering
	\includegraphics[width=0.45\textwidth]{eps/chan_extend.eps}
	\caption{channel extend machanism}
	\label{fig:spmckern:extend}
\end{figure}

%extension
Initially, as show in Figure \ref{fig:spmckern:extend}(a), the runtime will not allocate the actual page frames for \code{CHAN} area but map it to a special page frame--- anchor page table(\codet{Anchor1}), which is shared by the producer and consumer.
At the same time, ench entry in the \code{ptab} of the producer and the consumer will point to the corresponding Anchor1' entry. 

%That is each entry(pte) in \code{ptab} points to the address of Anchor1(Anchor1 in Figure\ref{fig:spmckern:extend}(a)).
When the producer sends data, the runtime attempts to write the sened data into the \code{CHAN} area, which will trigger a pagefault. 
%writing the data to CHAN, a pagefault will take place.
Then the pagefault handler will allocate a page frame(\code{page1}) for the producer and the corresponding \code{pte} in \code{Anchor1} will be updated to point to the page(\code{page1}). 
%After that, the runtime can copy the sended data to the page(\code{page1}).
%Then producer can locate the page by \code{pte} and the data need to send will copy to this \code{page1} lastly.
After that, the consumer can locate the written page frame\code{page1} by the anchor page(\code{Anchor1}, and then read data from \code{page1}.


If the channel buffer(\code{CHAN}) area is full, the producer need to wait until the data in the channel buffer been removed by the consumer, traditionally, which is limited the performance and throughput.
To avoid the producer waiting, we design an \codet{extend} mechanism which allows the producer send data 
without caring the buffer full.
The mechanism will remap \code{CHAN} area to the new page frames to another anchor(Figure  \ref{fig:spmckern:extend} (b)). 
Then the runtime will allocate the page frame(\codet{page2}) for the producer, which will be recorded in corresponding \code{Anchor2}'s PTE.
To record and trace generations of page frames among old page frames and new page frames, a extension PTE is introduced.
And the consumer will not be distracted by the extent mechanism and it can read the data from the old page frame(\code{page1}).
When the consumer receives all data from the old page frames, it can locate the new page frames by the extension-PTE.
The older page frames decrease their reference counts and are freed automatically when the counts reach zero.


%and consumer can read the old page frames at the same time.
%In order to reach a unbounded buffer,

%but just remap \code{CHAN} to another anchor \code{Anchor2},
%so that the producer can write the page afterwards.
%To record and trace generations of page frames among old page frames 
%and new page frames, a extension PTE is introduced.
%That mean,
%it can call extend primitive to remap channel buffer to new page frames,
%without changing the old page that consumers may still require. 
%Consumer can use the extension \code{pte} to locate the new
%After a consumer receive the old page from the channel, 
%it can call extend to find the new page frames sended by the
%producer.
%The older page frames decrease their reference counts and
%are freed automatically when the counts reach zero.


%a special anchor extension page is introduced and shared between producer thread and consumers consumer.
%Initially a sequence of pages in channel buffer 
%are mapped to a \codet{anchor} page table (Anchor in Figure
%\ref{fig:spmckern:extend}), 
%in which each page has a corresponding page table entry (PTE).
%Upon a producer page fault, the fault handler will allocate a real page frame and update the faulting page with a
%writable producer mapping so that the producer can write the
%page afterwards.
%When a thread want to send without waiting, 
%it can call extend primitive to remap channel buffer to new page frames,
%without changing the old page 
%that consumers may still require. 


In conclusion, in order to avoid the producer waiting when the shared-channel is full, we design an unbounded size of communication buffer for channel, which is a prominent feature.
We will detail how this feature will be used in \myds in Section 4.
%That means a sender can send any number of messages without blocking or waiting.
%As a result, system can achieve high throughput.
%Unboundedness goal is the key to achieve high throughput.








