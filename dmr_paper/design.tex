\section{Design of thread program model}
%这一部分，首先，我们将详细分析Phoenix较差scalability的根本原因，然后针对Phoenix scalability存在的challenge，我们提出可行的解决方案，即构造一种具有较好scalability的thread model。然后详细解释这种新的线程模型的详细设计方案，最后具体的实验部分在section 6
{\color{red}In this section, we will search the main factor of Phoenix's bad scalability.
Then we present a new thread model, which has good scalability.
}


\subsection{Scalability of Phoenix}
%描述Phoenix 的 pool scalability
In Phoenix, programs are written to start as many threads as the system has cores,
As indicated by figure\ref{fig:phoenix:speedup}, 
when the system is larger than their scalability limit 
adding more threads might actually scale negatively.
The time of completing a workload for one core increases 
when there are more cores in the system. 
The trend of this curve suggests that
the parallel scalability of Phoenix is poor.

%使用perf来收集热点函数的执行时间，实验结果显示，在32核情况下，(这里可以给出一个图，16和32核下的热点函数分别是什么，占用多少的百分比分别又是多少？)
In order to understand the scalability behavior, 
Perf\cite{} is exploited to collect execution time information
on the function basis. 
Figure\ref{fig:phoenix:spinlock} shows the percent of \_\_ticket\_spin\_lock of each benchmark.
Histgram on 16cores and 32 cores
show that \_\_ticket\_spin\_lock is one function 
which have largest execution time with 71.25\% and 40.15\% respectively. 
Experimental results show that Phoenix suffer from serious lock contention
when the core count exceeds 8.
That means Phoenix waster most of time to wait but not actual work.
And it can't fully use mulitcores resource.
In fact, Phoenix takes two strategies to avoid multiple threads operate the global matrix concurrently.
The spinlock is caused by contention the shared structure data multiple threads in Linux kernel.
\begin{figure}[!h!t]  
    \centering
    \includegraphics[width=0.35\textwidth]{eps/phoenix_spinlock.eps}
    \caption{Phoenix spinlock percent}
    \label{fig:phoenix:spinlock}
\end{figure}


%Phoenix采取了划分和barrier的方式，以避免多个线程对共享区域的的竞争，为什么还会存在如此高的spinlock呢，
%\grayt{There is a sense in the community that traditional kernel
%designs won’t scale well on multicore processors: that
%applications will spend an increasing fraction of their time
%in the kernel as the number of cores increases.
%To understand the Linux scalability
%behavior, we analyze the related implementation of Linux
%kernel and exploit performance tools to identify scalability
%bottlenecks.
%}
Call-graph information and source code analysis show that 
\_\_ticket\_spin\_lock is caused by pagefault.
The mmap() system call is used to read in input data. 
Once the user passes the pointer to the mmap()ed region as a runtime input, 
multiple threads will concurrently cause page-fault in the input data
while invoking their map functions.
When a large multi-threaded computing job 
that causes a lot of parallel page-faults, 
these page-faults will all run into contention on the mm\_sem semaphores.
Semaphores are sleeping locks 
and may run into convoying problems 
where waiting threads may 
get stuck at the end of the wait queue for a long time.\cite{Andi2009lmulticore}
Thus, spin lock 
contention degrades the parallel scalability performance of 
the benchmark. 

Data structure private locks can be a problem 
if the data structure is shared by multiple threads. 
A standard example here are the mm\_sem read-write semaphore 
that protects the list of mappings in a process and 
the pagetable\_lock that protects the pagetable state of a process. 
These locks are local to a process’ address space. 
\bluet{In Phoenix, there is one master process,
and map worker or reduce worker are threads belong to the master process}. 
However when the process is using multiple threads 
then these threads will be able to access the address space in parallel,
which can cause contention on these locks.
%Call-graph information
%and source code analysis show that the two functions are
%called when adding a virtual memory address range into the
%process address space or deleting a virtual memory address range. 


One reason that widely used operating systems 
use a lock on the address space is 
that they use complex index data structures to guarantee O(log n)
lookup time when a process has many mapped memory
regions. Linux uses a red-black tree for the regions\cite{linux}. 
Because the data structures require rebalancing 
when a memory region is inserted, 
they protect the entire data structure with a single lock.
The lock is local to a process address space.
when the process is using multiple threads 
then these threads will be able to access
the address space in parallel, 
which can cause contention on the lock.
%随着核数的增多，这种问题会更加突出，最好导致较差的scalability
\bluet{As the increasing of cores number, 
the scalability will be bad.
}

%Unfortunately these programs are often written to start as many threads as the system has CPUs, 
%but when the system is larger than their scalability limit adding more threads might actually scale negatively.
%The first measure is to limit them to the maximum number of threads that they can successfully scale to.
%This of course leaves some of the CPUs idle. 

%Performance data reveal that two functions vma link() and
%unlink file vma() have the largest execution time (46.02%
%and 49.97\%) and lock contention. Call-graph information
%and source code analysis show that the two functions are
%called when adding a virtual memory address range into the
%process address space or deleting a virtual memory address range. 
%When multiple slave processes call mmap() or unmap() concurrently, the 
%memory mapped file address range should be added into or 
%deleted from each slave process address space. However, 
%the same spin lock protecting the memory mapped file 
%address range should be held or released. Thus, spin lock 
%contention degrades the parallel scalability performance of 
%the benchmark. 



%The difference between processes and threads under Linux 2.4 is that threads share more parts of their state (address space, file handles etc) than processes, 

\subsection{Scalable thread model}
%这一部分，我们不提生产消费模型，
%为什么要进行地址空间隔离，通过新的线程模型，我们实现了地址空间隔离，隔离之后有什么好处？相比Phoenix，我们这种新的模型会不会带来开销。

Our goal is to make pagefaults scale to 
large numbers of cores.
There are many way to achive this target such as \cite{Clements2012Scalable}\redt{...}.
However, We don't want to change the implementation of operating system, 
but just provoid a easy-to-use scalable thread model.
This requires addressing a basic problem,
how allow page faults to run concurrently 
to eliminate on the per-process read/write lock.
In fact, if applications use processes instead of threads can avoid a single, 
shared address space, but this complicates sharing.
To achieve our goal, we persents a new concurrent address space design 
that eliminates the above sources of contention by applying a new program model and by introducing channel, a way to share data between threads.
We aim at providing an race-free programming abstraction 
to support scalable MapReduce.
%We aim at providing an easy-to-use programming abstraction 
%to support scalable MapReduce.


With this target, we propose a new thread programming model \myth(Scalable thread).
%We propose a new thread programming model to support efficient pipeline parallelism.
%新的线程模型与传统的Pthread模型的主要不同在于：(1)每个线程拥有自己独立的地址空间，这样可以避免多个线程对mm\_struct结构的竞争。(2)线程之间有一个共享的通道，同于线程数据的共享。
\myth is C library-based and 
thread in \myth run in separate memory spaces.
Threads in Pthreads share the address space of the process that created it, 
while threads in \myth have their own address space,
meaning each thread has a mm\_struct.
Therefore, thread no need contend with others thread for lock.
On the other hand,
Threads based on share space can directly communicate with other threads of its process; 
While \myth must use interprocess communication to communicate with the other threads.
We provoide a share channel for threads to communicate.


\label{sec:pm:thread}
\begin{figure}[htpb]
\input chanapi.tex
\caption{Main functions of \myds thread API.}
\label{fig:api:thread}
\end{figure}

%mapreduce中是如何使用这个简易的模型进行编程和实现的，这个模型潜在的开销是什么
Figure\ref{fig:api:thread} lists main function of managing threads and channels in \myth.
At initial stage of \myds, the master thread invoke 
\codet{thread\_alloc} to create map and reduce workers, 
and then creates and sets up channels for communication between each pair of map worker and reduce worker by invoking \codet{chan\_alloc}.
After that, the master starts all workers by invoking \codet{thread\_start}.
In \myds, all channels have one producer 
to send message and one consumer to receive message, 
which is a producer-consumer model.
The master invoke \codet{chan\_setprod} and \codet{chan\_setcons} 
to set map worker as producer and reduce worker as consumer, respectively.
(We will describe it in Section 4). 

Though, using \myth can decrease the overhead of contention,
it also take some extra overhead comparing to Phoneix. 
The extra overhead focus on initialization(section 5). 


\subsection{Design of the Channel}
%channel的底层实现，以及它无限制的映射机制，想说明的问题是：不需要等待，且没有过多的malloc和free操作带来的开销。
Once the channel relationships are set up, 
map workers can invoke \codet{chan\_send} to send key-value to corresponded channel,
and the reduce workers can receive from the channel by \codet{chan\_recv}.
In order to avoid map waiting when the channel buffer is full,
we design an unbounded size of communication buffer for channel.
That means a sender can send any number of messages without blocking or waiting.
As a result, system can achieve high throughput.
%Unboundedness goal is the key to achieve high throughput.
%这个特性的好处


%无边界的channel的实现，依赖于底层的extend机制，
%它允许将channel buffer区域重新映射到一块新的物理地址，
%并且不影响consumer对旧的物理的读取。
%channel buffer对应producer和consumer地址空间中一块区域，
%我们称之为\codet{CHAN}区域，有一个pagetable（\codet{ptab}）用于管理该区域的物理地址映射。
\begin{figure}[!h!t]  
	\centering
	\includegraphics[width=0.5\textwidth]{eps/chan_extend.eps}
	\caption{channel extend machanism}
	\label{fig:spmckern:extend}
\end{figure}
In order to reach a unbounded buffer,
We design an extend mechanism 
which allows remap channel’s buffer to a new page frames,
and consumer can read the old page frames at the same time.
In our design, channel buffer is a virtual memory area \code{CHAN}
in the producer and consumer address space.
There is a pagetable(\code{ptab}) to store the mapping between virtual addresses and physical addresses, with each mapping as a corresponding page table entry(\code{PTE}).

%extension
Initially, the system will not allocate the actually page frames for \code{CHAN} area but a a special anchor page(\codet{Anchor1}),
which is shared between producer and consumer.
That is each entry(pte) in \code{ptab} points to the address of Anchor1.
When the producer need send message, 
writing the data to CHAN, a pagefault will take place.
Upon a producer page fault, the fault handler will allocate a real page frame(\code{page1}) and the \code{Anchor1}'s \code{pte} point to the page. 
Then producer can locate the page by \code{pte} and the data need to send will copy to this \code{page1} lastly.
After sending, consumer can locate the written page\code{page1} by pte 
and read it.

If the channel buffer(\code{CHAN}) area is full, 
producer no need to writing, 
but just remap \code{CHAN} to another anchor \code{Anchor2},
so that the producer can write the page afterwards.
To record and trace generations of page frames among old page frames 
and new page frames, a extension PTE is introduced.
That mean,
it can call extend primitive to remap channel buffer to new page frames,
without changing the old page that consumers may still require. 
Consumer can use the extension \code{pte} to locate the new
After a consumer receive the old page from the channel, 
it can call extend to find the new page frames sended by the
producer.
The older page frames decrease their reference counts and
are freed automatically when the counts reach zero.






 
%a special anchor extension page is introduced and shared between producer thread and consumers consumer.
%Initially a sequence of pages in channel buffer 
%are mapped to a \codet{anchor} page table (Anchor in Figure
%\ref{fig:spmckern:extend}), 
%in which each page has a corresponding page table entry (PTE).
%Upon a producer page fault, the fault handler will allocate a real page frame and update the faulting page with a
%writable producer mapping so that the producer can write the
%page afterwards.
%When a thread want to send without waiting, 
%it can call extend primitive to remap channel buffer to new page frames,
%without changing the old page 
%that consumers may still require. 






%\begin{figure*}[htpb]
%	\centering
%	\subfigure[when the channel buffer is not full]{
%		\includegraphics[width=0.35\textwidth]{eps/chan_extend1.eps}
%		\label{fig:dmr:chan:extend1}
%	}
%	\subfigure[the channel buffer is full, remapp the buffer to a new]{
%		\includegraphics[width=0.45\textwidth]{eps/chan_extend2.eps}
%		\label{fig:dmr:chan:extend2}
%	}
%	\caption{Extend mechanism of channel in \myth}
%	\label{fig:time}
%\end{figure*}

In the follow section, we will describe \myds, 
which is based on \myth.
we focus on how the producer-consumer model and unbounded channel be used in \myds.








