\section{Introduction}
\label{sec:intro}

%%需要调研的部分：解答的问题，为什么我们关注多核的scalability问题。随着分布式系统的广泛普及，企业和用户的通用的做法是通过堆积更多的机器来获取更快的处理效率和速度。但是机器堆积的越多，网络带宽，耗电也越来越多，如何充分利用单台机器的多核资源成为很重要的课题。

%一方面，随着多核机器的广泛普及，如何充分利用多核资源成为非常重要的课题，另一方面，现有的很多我们希望充分利用单台机器上的资源，



Parallel programming is becoming more and more popular
because of its potential to improve performance, especially in
multicore architectures. In the past, computer software has
been written using serial computation concepts which is
usually less efficient than multithreaded parallel computation.


As the prevalence of multicore chips,
it is foreseeable
that tens to hundreds (even thousands) of cores on a single chip
will appear in the near future\cite{Borkar2007core}.
While utilizing multicore sources is still challenging
because of the difficulties of parallel programming.
%另一方面，现在流行的分布式环境

MapReduce\cite{dean2004mapreduce} 
is a promising programming model for clusters
to perform large-scale data processing
in a simple and effecient way.
In most cases, programmers only need to implement two interfaces:
Map, which processes the input data and converts it into a num-
ber of key/value pairs; and Reduce, which aggregates values in the
key/value pairs according to the key.
And the programmer dose no need to control synchronization 
and schedule tasks manually.

Parallel computing concept is a rapidly-growing field,
but many problems do not currently scale well onto multiple
processors. In many cases, much of the benefit of parallelism is
lost because of inefficiencies in the parallel program structure


Ideally, adding more MPI processes to the simulation
would bring about a linear decrease in execution time and
power consumption.
However, the benefits of adding more
processes will be reduced due to overhead associated with the
additional processes – the communication delay and
 dependencies among the process [5, 6]

While initially MapReduce is implemented on clusters, Ranger
et al. have demonstrated the feasibility of running MapReduce
applications on shared memory multicore machines with 
Phoenix\cite{ranger2007phoenix}.
Other libraries such as Metis\cite{mao2010metis} 
and Tiled-mapreduce\cite{chen2010tiled},
also show that MapReduce is a promising programming model 
for multicore platforms to take full advantage of  
processing resources.
Phoenix uses the pthread library to assign tasks 
among CPU cores and relies on
shared memory to handle inter-task communications.

Compared to the cluster version, MapReduce on multicore is able to take ad-
vantage of fast inter-task communications in shared memory, thus
avoids the expensive network communications among tasks.

Though Phoenix has demonstrated the applicability of running
MapReduce on multicore, the scalability of Phoenix is limited 
due to its way of processing input data and Pthreads library.
POSIX Threads (usually referred as Pthreads)
is a POSIX standard for threads.
As a result, the performance will be better 
when the number of cores increases from 1 to 4, 
while the performance will be worse if using more than 4 cores.  
Hence, with the continuously increasing
number of cores, it can easily cause resource pressures on the
runtime, operating systems and the CPU caches, which could sig-
nificantly degrade the performance. 

To remedy the above problems, this paper presents a modified model of \myds, 
that can efficently support MapReduce applications.



\myds reserves the similar DMR programing interfaces as well.
Specifically, this paper make the following contributions:
\begin{enumerate}
\item Threads communicate with each other by DetMP.
\item To improve preformance, we use private buffer rather then shared buffer. Eche worker has its own set of buffers. 
\item we explore different implementation of buffer: hash buffer or array buffer.
\end{enumerate}

%介绍文章的组织结构
In order to ground our discussion, we present an overview
of MapReduce architecture in Section 2. We
then develop the design of HOP’s pipelining scheme in
Section 3, keeping the focus on traditional batch process-
ing tasks. In Section 4 we show how HOP can support
online aggregation for long-running jobs and illustrate
the potential benefits of that interface for MapReduce
tasks. In Section 5 we describe our support for continu-
ous MapReduce jobs over data streams and demonstrate
an example of near-real-time cluster monitoring. We
present initial performance results in Section 6. Related
and future work are covered in Sections 7 and 8.

This paper is organized as follows. In section 2, we review
the background. Section 3 briefly describes the problems.
Section 4 presents the experimental setup. Section 5 discusses
the results in terms of execution time and total power
consumption. Finally, Section 6 concludes the paper.


