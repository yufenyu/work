\section{Implementation and Runtime}
In this section we discuss our extensions to 
the MapReduce programming model and shows the major changes to runtime to 
support \myds
to support pipelining between Map and Reduce (Section 3.2). 
We describe how our design supports
piplelining (Section 3.3), and discuss the buffer design (Section 3.4). 
Our focus here is on Producer-Consumer model; 
We defer performance results to Section 5.
e


\subsection{Execution flow}
\begin{figure}[!h!t]  
    \centering
    \includegraphics[width=0.5\textwidth]{eps/dmr_workflow.eps}
    \caption{The workflow of DMR}
    \label{fig:dmr:workflow}
\end{figure}

%不同与Phoenix，DMR将任务队列静态的划分给每个map worker，这样做的好处是，可以避免多个map worker对对任务队列操作从而产生锁的开销。
\myds's implementation of MapReduce closely resembles Phoenix’s. 
There is a single master managing a number of slaves worker. 
Figure\ref{fig:dmr:workflow} illustrates the workflow of \myds,
including three main phases: map, reduce, and merge. 
At the beginning, a split function divides the input
data across workers. 
On multi-core CPUs, a worker is handled by one thread. 
A worker usually needs to process multiple input elements. 
Thus the map function is applied to the input elements one by one. 
Such an operation of applying the map function for an input element is 
called a map operation. 
Each map operation produces intermediate key-value pairs. 
Then a partition function is applied to these key-value pairs. 
Then in the reduce phase, each reduce operation
applies the reduce function to a set of intermediate pairs
with the same key. 
Finally the results from multiple reduce workers are merged and output.

Compared with existing work, 
our \myds has two main designment to improve its performance and scalability.
On the one hand, We propose produce-conumse model to 
pipeline map and reduce phase. 
ie. map worker as a producer insert key-value into local buffer,
and reduce worker as a consumer will fetch key-value from buffer
when the buffer is full. 
Reduce phase can start without waiting the Map phase finished.
On the other hand, \myds implemente a worker as single-threaded process 
instead of as a single multi-thread process.
Then workers will be in the isolated address space 
avoiding contention on a single lock per shared address space.

%The map phase reads the task’s split from task queue,
%parses it into key-value pairs, and applies
%the map function to each record.
%Intermediate keys are assigned
%to reducers by applying a partitioning function
%A spill of the in-memory buffer involves first sorting
%the records in the buffer by partition number and then by
%key.

%After receiving its partition from all map outputs, the
%reduce task enters the sort phase. The map output for
%each partition is already sorted by the reduce key. The
%reduce task merges these runs together to produce a sin-
%gle run that is sorted by key. The task then enters the
%reduce phase, in which it invokes the user-defined reduce
%function for each distinct key in sorted order, passing it
%the associated list of values.
%
%We addressed these issues by buffering the mapper output until
%it reaches a certain record threshold.
%When the record threshold isreached, 
%the mapper sends the output to a reducer. 
%Next, Reducer receives its partition from all map, 
%and enters the sort phase.
%将之前对combiner和merge阶段的特征进行简单的总结
{\bf Combiner.}
In order to maximally reduce memory pressure due to intermediate key-value
storage.%通道的数据流通量(map和reduce见传递的数据量)
%做法，开销，优势，我们的特点，可以在reduce阶段进行combiner
The imbalance of tasks can be solved by dynamic scheduling in the Map phase. 
%%事实上，为了防止出现数据倾斜的问题，即map阶段的很多key都发送到一个reduce，导致某个reduce有过多的动态内存分配，甚至可能出现内存不够的情况，我们可以对reduce的数据做局部的combiner
%However, in the Reduce phase, 
%as all values for the same key must be in one reduce task, 
%it is not always feasible to generate a large number reduce tasks for
%%dynamic scheduling.

{\bf merge.}
Each Reduce generates a set of output key/value
pairs, and the library’s Merge phase sorts them by key to
produce the final output. 
%做法，作用，我们的优化，不需要进行reduce和merge的阶段，应当尽量避免，以降低时间的开销

\subsection{Pipelined execution}
Pipelined map and reduce has been adopted 
in the MapReduce framework for distributed computing\cite{Condie2010MapReduce,}. 
Condie et al. show that since the intermediate data is delivered to
downstream operators more promptly, 
it is able to improve resource utilization.

%In general MapReduce programming model, 
While in Phoenix, (To avoid mulit map and reduce contention the same area, )
there is a strict barrier between the Map and Reduce phases: 
the workers in one phase can only be started 
until all workers in the previous phase has been finished. 
Hence, the execution time of a job is determined by the
slowest worker in each phase. 
A downstream dataflow element can begin consuming data
before a producer element has finished execution, which can
increase opportunities for parallelism, improve utilization,
and reduce response time.
On the other hand, MapReduce workloads
are an ideal candidate for pipelining as the user-defined
map functions are usually computation-intensive, while the
reduce phase to construct the global container is memory
intensive\cite{talbot2011phoenix++}.
Overlapping the
computation-intensive and memory-intensive workloads 
can effective improve the overall hardware resource utilization.

%我们设计了一个生产者和消费者模型，用于map和reduc阶段的流水并行，有两个重要的数据结构：map worker的buffer池，reducer worker的全局buffer。每个map worker拥有一个私有的buffer池，当key-value产生后，通过partition函数插入到对应的buffer中。其中每个buffer对应一个reduce worker，reduce worker轮循的从各个map的buffer池中取key-value，并调用reduce函数进行计算；每个reduce拥有一个私有的全局buffer，用于存放reduce处理后得到的全局结果。
We design a producer-consumer model to pipeline the
map and reduce phases.
There are two major data structures,
which are local buffers pool for each map worker and 
a global buffer for reduce worker.
Specifically, 
each map worker has a local buffers pool and 
each buffer for a corresponding reduce worker.
Partition function will be used to push key-value into a corresponding
buffer.
When the buffer threshold is reached,
the corresponding reduce worker can read record in the buffer.
One reduce worker will get key-value from each map by Round-Robin and 
merge the key-value pairs to the global buffer.


%如同Phoenix, 默认情况下，buffer使用hash table来实现，事实上，在我们的模型中，使用array来实现具有更好的性能。下面的章节会详细解释原因。
Defaultly, the buffer is a hash table as Phoenix.
While this technique is more effective for the array buffer
container than the hash buffer. 
We will explain the advantage of array buffer in section 3.2.1.
MRPhi\cite{lu2013mrphi} is also use producer-consumer model to 
pipeline the map and reduce phases. 
There are partitions queues for each reduce worker.
While we don't use queues, mapping will be used in DMR(section 3.2.2).

%(The motivation is that the map function defined by users
%usually performs heavy computation. But the reduce phase
%contains many memory accesses in which the major work is to
%construct the global container. 
% )


\subsubsection{Producer-Consumer model}
The pipelined map and reduce are implemented using a
producer-consumer model. 
The map and reduce workers are the producers and consumers, respectively.
Specially, the map worker generate key-value and 
put it into the buffer.
At the same time, 
the reduce worker is consuming the key-value.
%As the buffer is a shared region of memory in tradition,
%either a map worker or a single reduce worker,
%can access the structure at any given time.
%使用queue
%In fact, producer-consumer model implementations
%often use a shared queue.
MRPhi\cite{lu2013mrphi}design a producer-consumer model 
to pipeline the map and reduce phases. 
%There are three major data structures, 
%which are local hash tables, a global hash table, 
%and partition queues. Specifically, 
Each map worker has a local hash table. 
When a local hash table is full,
key-value pairs stored in this table are partitioned and
pushed into corresponding queues. 
Meanwhile, one reduce
worker works on one queue to merge the key-value pairs to
the final global hash table.

% MRPhi的缺点，（1）多对一的竞争问题，（2）队列的管理问题

However, there are two main defects of producer-consumer model in MRPhi.
Firstly, The queue used in MRPhi is a many-to-one queue.
Multiple map worker will append data to the tail of queue {\color{gray}contendly}. 
when one thread is appending, any thread waiting to append is stalled.
That means more time spent in waiting lock.
Secondly, They don’t describe, however, how to manage the queue,
which using a allocted space or by dynamic memory allocation.
if using a allocated space, when the queue is full, 
the producer will blocking waiting for a slot in the queue.
it is bad for parallelion excution of Map and Reduce.
if dynamic memory allocation, 
map no need waiting, while there will have a large number of malloc and free,
which causing overhead.

\begin{figure}[!h!t]  
    \centering
    \includegraphics[width=0.4\textwidth]{eps/dmr_channel.eps}
    \caption{Produce-Consume model in SMR}
    \label{fig:dmr:pc-model}
\end{figure}
%为了避免上述producer-consumer模型的问题，我们重新设计，并且希望它
{\color{red} our tagert of designing the produce-consume model}\\

Figure.\ref{fig:dmr:pc-model} shows producer-conusmer model in our \myds.  
There is a one to one channel between map worker and reduce worker, 
avoiding the contention of multiple map workers.
When the buffer is full, map worker send the buffer to channel, 
at the same time, 
reduce worker receive data from channel, and copy it to local buffer.
on the other hand,
we use the a special mapping to allow producer and consumer parallelization effectively, 
without waiting when the queue is full, or malloc and free operations.
When the local buffer is full,
it send the data to channel and then use the buffer again.
That means map no need wait and there is no many malloc and free operations.
Figure\ref{fig:dmr:pc-model}
Thus, this parallelization effectively decouples the behavior of proucer and conusmer, and allows them to be overlapped 
without many malloc and free.

In Section\ref{} we will present the design of Channel.

%has to maintain a queue of threads waiting for that monitor
%It is important for databases and large web and proxy servers to map files into memory instead of having a buffer and reading the file contents into the buffer. If you map the file into memory directly, the operating system has more memory left for I/O buffering.



%{\color{gray}
%In the shared-memory model, a region of memory that is shared
%by cooperating processes is established. 
%Processes can then exchange information 
%by reading and writing data to the shared region.
%Shared memory can be faster.
%since in shared-memory systems, system calls are required only to establish shared-memory regions. 
%Once shared memory is established, all accesses are treated
%as routine memory accesses, and no assistance from the kernel is required.
%}
%%SPMC区域的建立
%Interprocess communication using shared memory requires communicating
%processes to establish a region of shared memory. Typically, a shared-memory
%region resides in the address space of the process creating the shared-memory
%segment. Other processes that wish to communicate using this shared-memory
%segment must attach it to their address space. 
%
%%SPMC区域的使用，对应buffer
%To allow producer and consumer processes to run concurrently, we must have
%available a buffer of items that can be filled by the producer and emptied by
%the consumer. This buffer will reside in a region of memory that is shared by
%the producer and consumer processes.（http://bulk.fefe.de/scalability/）



\input buffer


